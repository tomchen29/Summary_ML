{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author @Tonghong Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sns.load_dataset('iris')\n",
    "X = df.iloc[:, :-1]\n",
    "y = df['species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\OS_App\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### standardize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std_train = StandardScaler().fit_transform(X_train)\n",
    "X_std_test = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGDClassifier in sklearn package is optimized on finding local minima, and thus usually performs pretty bad on the iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=1000, n_iter=None,\n",
       "       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=0.001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', penalty='elasticnet', \n",
    "                    alpha=0.0001, l1_ratio=0.15, max_iter=1000, tol=1e-3)\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_std_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auto-Selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='saga',multi_class='multinomial', penalty='l2',\n",
    "                         tol = 1e-03, max_iter=1000)\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\OS_App\\anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([  6.73795e-03,   1.33560e-02,   2.64743e-02,   5.24776e-02,\n",
       "         1.04021e-01,   2.06192e-01,   4.08715e-01,   8.10158e-01,\n",
       "         1.60590e+00,   3.18322e+00,   6.30981e+00,   1.25073e+01,\n",
       "         2.47921e+01,   4.91431e+01,   9.74118e+01,   1.93090e+02,\n",
       "         3.82745e+02,   7.58679e+02,   1.50386e+03,   2.98096e+03]),\n",
       "           class_weight=None, cv=5, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1.0, max_iter=100, multi_class='multinomial',\n",
       "           n_jobs=1, penalty='l1', random_state=None, refit=True,\n",
       "           scoring='neg_log_loss', solver='saga', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "Cs = np.exp(np.linspace(-5, 8, 20))\n",
    "nfold = 5\n",
    "clf = LogisticRegressionCV(Cs=Cs,cv=nfold, penalty=\"l1\", solver='saga', multi_class=\"multinomial\", \n",
    "                            scoring=\"neg_log_loss\", refit=True)\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=50, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=50, metric='minkowski', p=2) # a small n_neighbors may result in overfitting\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1, verbose=True)  # Higher C <=> fewer margin violations but ends up with a smaller margin\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='poly', degree=3, C=1) # degree: only valid for the polynomial kernel function\n",
    "clf = svm.SVC(kernel='rbf', C=1) \n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=10, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=5, min_samples_leaf=10)\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"554pt\" height=\"433pt\"\r\n",
       " viewBox=\"0.00 0.00 554.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-429 550,-429 550,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.023529\" stroke=\"black\" d=\"M262,-425C262,-425 133,-425 133,-425 127,-425 121,-419 121,-413 121,-413 121,-354 121,-354 121,-348 127,-342 133,-342 133,-342 262,-342 262,-342 268,-342 274,-348 274,-354 274,-354 274,-413 274,-413 274,-419 268,-425 262,-425\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal_width &lt;= &#45;0.559</text>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.666</text>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 120</text>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [38, 42, 40]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M168,-298.5C168,-298.5 71,-298.5 71,-298.5 65,-298.5 59,-292.5 59,-286.5 59,-286.5 59,-242.5 59,-242.5 59,-236.5 65,-230.5 71,-230.5 71,-230.5 168,-230.5 168,-230.5 174,-230.5 180,-236.5 180,-242.5 180,-242.5 180,-286.5 180,-286.5 180,-292.5 174,-298.5 168,-298.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"119.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"119.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 38</text>\r\n",
       "<text text-anchor=\"middle\" x=\"119.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [38, 0, 0]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"119.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 0</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.438,-341.907C162.935,-330.652 154.779,-318.418 147.237,-307.106\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.071,-305.046 141.611,-298.667 144.246,-308.929 150.071,-305.046\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"136.708\" y=\"-319.481\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.047059\" stroke=\"black\" d=\"M341,-306C341,-306 210,-306 210,-306 204,-306 198,-300 198,-294 198,-294 198,-235 198,-235 198,-229 204,-223 210,-223 210,-223 341,-223 341,-223 347,-223 353,-229 353,-235 353,-235 353,-294 353,-294 353,-300 347,-306 341,-306\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"275.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal_length &lt;= 0.655</text>\r\n",
       "<text text-anchor=\"middle\" x=\"275.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"275.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 82</text>\r\n",
       "<text text-anchor=\"middle\" x=\"275.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 42, 40]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"275.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M224.562,-341.907C230.43,-333.105 236.698,-323.703 242.759,-314.612\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"245.851,-316.283 248.486,-306.021 240.027,-312.4 245.851,-316.283\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"253.389\" y=\"-326.835\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.901961\" stroke=\"black\" d=\"M254,-187C254,-187 129,-187 129,-187 123,-187 117,-181 117,-175 117,-175 117,-116 117,-116 117,-110 123,-104 129,-104 129,-104 254,-104 254,-104 260,-104 266,-110 266,-116 266,-116 266,-175 266,-175 266,-181 260,-187 254,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"191.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal_width &lt;= 0.315</text>\r\n",
       "<text text-anchor=\"middle\" x=\"191.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.162</text>\r\n",
       "<text text-anchor=\"middle\" x=\"191.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 45</text>\r\n",
       "<text text-anchor=\"middle\" x=\"191.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 41, 4]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"191.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M246.357,-222.907C239.971,-214.014 233.148,-204.509 226.558,-195.331\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"229.267,-193.103 220.592,-187.021 223.581,-197.185 229.267,-193.103\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.972549\" stroke=\"black\" d=\"M421,-187C421,-187 296,-187 296,-187 290,-187 284,-181 284,-175 284,-175 284,-116 284,-116 284,-110 290,-104 296,-104 296,-104 421,-104 421,-104 427,-104 433,-110 433,-116 433,-116 433,-175 433,-175 433,-181 427,-187 421,-187\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"358.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal_width &lt;= 0.852</text>\r\n",
       "<text text-anchor=\"middle\" x=\"358.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.053</text>\r\n",
       "<text text-anchor=\"middle\" x=\"358.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 37</text>\r\n",
       "<text text-anchor=\"middle\" x=\"358.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 36]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"358.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 2</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>2&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M304.296,-222.907C310.606,-214.014 317.348,-204.509 323.859,-195.331\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"326.823,-197.202 329.754,-187.021 321.114,-193.152 326.823,-197.202\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-68C109,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 109,-0 109,-0 115,-0 121,-6 121,-12 121,-12 121,-56 121,-56 121,-62 115,-68 109,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"60.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"60.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\r\n",
       "<text text-anchor=\"middle\" x=\"60.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 30, 0]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"60.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.72,-103.726C131.372,-94.2406 119.307,-84.1551 108.014,-74.7159\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110.255,-72.0276 100.338,-68.2996 105.766,-77.3984 110.255,-72.0276\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.635294\" stroke=\"black\" d=\"M248,-68C248,-68 151,-68 151,-68 145,-68 139,-62 139,-56 139,-56 139,-12 139,-12 139,-6 145,-0 151,-0 151,-0 248,-0 248,-0 254,-0 260,-6 260,-12 260,-12 260,-56 260,-56 260,-62 254,-68 248,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.391</text>\r\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 15</text>\r\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 11, 4]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 1</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.479,-103.726C195.085,-95.4263 195.725,-86.6671 196.338,-78.2834\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"199.829,-78.5281 197.067,-68.2996 192.848,-78.018 199.829,-78.5281\"/>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.890196\" stroke=\"black\" d=\"M395,-68C395,-68 306,-68 306,-68 300,-68 294,-62 294,-56 294,-56 294,-12 294,-12 294,-6 300,-0 306,-0 306,-0 395,-0 395,-0 401,-0 407,-6 407,-12 407,-12 407,-56 407,-56 407,-62 401,-68 395,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"350.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.18</text>\r\n",
       "<text text-anchor=\"middle\" x=\"350.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 10</text>\r\n",
       "<text text-anchor=\"middle\" x=\"350.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 9]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"350.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 2</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;7 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>6&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M355.521,-103.726C354.915,-95.4263 354.275,-86.6671 353.662,-78.2834\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"357.152,-78.018 352.933,-68.2996 350.171,-78.5281 357.152,-78.018\"/>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M534,-68C534,-68 437,-68 437,-68 431,-68 425,-62 425,-56 425,-56 425,-12 425,-12 425,-6 431,-0 437,-0 437,-0 534,-0 534,-0 540,-0 546,-6 546,-12 546,-12 546,-56 546,-56 546,-62 540,-68 534,-68\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"485.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"485.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 27</text>\r\n",
       "<text text-anchor=\"middle\" x=\"485.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 27]</text>\r\n",
       "<text text-anchor=\"middle\" x=\"485.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = 2</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;8 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>6&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M405.79,-103.726C416.686,-94.3318 428.264,-84.349 439.121,-74.9883\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"441.59,-77.4804 446.879,-68.2996 437.019,-72.1789 441.59,-77.4804\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1b110e766d8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=X.columns,\n",
    "                                class_names=y.unique().astype('str'), rounded=True, filled=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (with Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=1000, n_jobs=-1, oob_score=True,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier().fit(X_std_train, y_train), \n",
    "                            oob_score=True, n_estimators=1000, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95833333333333337"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Trees (Extremely Randomized Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier()\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier())\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequentially adding predictors, and fit the new predictor by the residual errors of its predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. **\n",
    "\n",
    "  $$Y = F(x) + \\epsilon$$\n",
    "\n",
    "**Step 2. **\n",
    "\n",
    "  $$\\epsilon = G(x) + \\epsilon_2$$\n",
    "\n",
    "  Substituting (2) into (1), we get:\n",
    "  \n",
    "  $$Y = F(x) + G(x) + \\epsilon_2$$\n",
    "    \n",
    "**Step 3. **\n",
    "\n",
    "  $$\\epsilon_2 = H(x)  + \\epsilon_3$$\n",
    "\n",
    "Now:\n",
    "  \n",
    "  $$Y = F(x) + G(x) + H(x)  + \\epsilon_3$$\n",
    "  \n",
    "Finally, adding weights of each predictor  \n",
    "  \n",
    "  $$Y = \\alpha F(x) + \\beta G(x) + \\gamma H(x)  + \\epsilon_4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost (Regularized Gradient Boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Documentation](http://xgboost.readthedocs.io/en/latest/)\n",
    "\n",
    "[tqchen github](https://github.com/tqchen/xgboost/tree/master/demo/guide-python)\n",
    "\n",
    "[dmlc github](https://github.com/dmlc/xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=10000,\n",
       "       n_jobs=-1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=5, n_estimators=10000, learning_rate=0.3,\n",
    "                            n_jobs=-1)\n",
    "clf.fit(X_std_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5605 - acc: 0.7583 - val_loss: 0.5228 - val_acc: 0.7667\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 0s 727us/step - loss: 0.5415 - acc: 0.7500 - val_loss: 0.5178 - val_acc: 0.7667\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 0s 740us/step - loss: 0.5449 - acc: 0.7583 - val_loss: 0.5129 - val_acc: 0.7667\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 0s 791us/step - loss: 0.5305 - acc: 0.7583 - val_loss: 0.5082 - val_acc: 0.7667\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 0s 785us/step - loss: 0.5117 - acc: 0.7917 - val_loss: 0.5038 - val_acc: 0.7667\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 0s 723us/step - loss: 0.5236 - acc: 0.7917 - val_loss: 0.4997 - val_acc: 0.7667\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 0s 723us/step - loss: 0.5102 - acc: 0.7917 - val_loss: 0.4952 - val_acc: 0.7667\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 0s 752us/step - loss: 0.5264 - acc: 0.7750 - val_loss: 0.4913 - val_acc: 0.7667\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 0s 794us/step - loss: 0.5194 - acc: 0.8083 - val_loss: 0.4873 - val_acc: 0.7667\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 0s 744us/step - loss: 0.5119 - acc: 0.7750 - val_loss: 0.4830 - val_acc: 0.7667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2ae9f1f60>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.rmsprop(lr=0.0001, decay=1e-6),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_std_train, y_train_categorical,\n",
    "          batch_size=10,\n",
    "          epochs=10, \n",
    "          validation_data=(X_std_test, y_test_categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prediction accuracy of the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_std_test, y_test_categorical, verbose=True)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font size = 5 color = 'purple'> CNN (with MNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Dense, BatchNormalization, Activation, MaxPooling2D, Dropout, Input, Lambda, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# import keras.backend as K\n",
    "# from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape([-1, 28, 28, 1])\n",
    "x_test = x_test.reshape([-1, 28, 28, 1])\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cnn(IMG_WIDTH=28, IMG_HEIGHT=28, CHANNEL=1):\n",
    "    \n",
    "    inputs = Input((IMG_WIDTH, IMG_HEIGHT, CHANNEL))\n",
    "    norm_input = Lambda(lambda x: x/255)(inputs)\n",
    "  \n",
    "    c1 = Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu')(norm_input)\n",
    "    c2 = Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu')(c1)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(0.25)(p2)\n",
    "   \n",
    "    c3 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(p2)\n",
    "    c4 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(c3)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(0.25)(p4)\n",
    "  \n",
    "    f = Flatten()(p4)\n",
    "    d1 = Dense(256, activation='relu')(f)\n",
    "    d1 = Dropout(0.5)(d1)\n",
    "  \n",
    "    outputs = Dense(10, activation='softmax')(d1)\n",
    "  \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 887,530\n",
      "Trainable params: 887,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnnmodel = model_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a image generator\n",
    "imageGenerator = ImageDataGenerator(featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        zoom_range = 0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the CNN model with image generator\n",
    "cnnmodel.fit_generator(imageGenerator.fit(x_train).flow(x_train, y_train, batch_size=128),\n",
    "                     epochs=20, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the CNN model \n",
    "cnnmodel.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 49 candidates, totalling 147 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 147 out of 147 | elapsed:    8.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('clf', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'clf__C': (0.1, 0.5, 1, 2, 5, 10, 20), 'clf__gamma': (0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('clf', svm.SVC(kernel='rbf', C=1, gamma=0.1))]) \n",
    "\n",
    "# this are the parameters that we want the grid search to scan through\n",
    "params = {'clf__C':(0.1, 0.5, 1, 2, 5, 10, 20), \n",
    "          'clf__gamma':(0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1)} # the format is: 'clf_' + parameter\n",
    "\n",
    "# use grid search cross-validation to fin the best parameters\n",
    "svm_grid_rbf = GridSearchCV(pipeline, params, n_jobs=-1,\n",
    "                            cv=3, verbose=1, scoring='accuracy') \n",
    "svm_grid_rbf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97499999999999998"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_rbf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma=0.75, kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " 'clf__C': 1,\n",
       " 'clf__cache_size': 200,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__coef0': 0.0,\n",
       " 'clf__decision_function_shape': 'ovr',\n",
       " 'clf__degree': 3,\n",
       " 'clf__gamma': 0.75,\n",
       " 'clf__kernel': 'rbf',\n",
       " 'clf__max_iter': -1,\n",
       " 'clf__probability': False,\n",
       " 'clf__random_state': None,\n",
       " 'clf__shrinking': True,\n",
       " 'clf__tol': 0.001,\n",
       " 'clf__verbose': False,\n",
       " 'memory': None,\n",
       " 'steps': [('clf', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma=0.75, kernel='rbf',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False))]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid_rbf.best_estimator_.get_params() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84615385,  1.        ,  0.75      ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  0.91666667,  1.        ,  1.        ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf, X_std_train, y_train, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_std_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[10  0  0]\n",
      " [ 0  9  2]\n",
      " [ 0  0  9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_std_test))))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percision, Recall and F-1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: \t 0.9455\n",
      "Recall Score: \t\t 0.9333\n",
      "F1 Score: \t\t 0.9333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print(\"Precision Score: \\t {0:.4f}\".format(precision_score(y_test, clf.predict(X_std_test), average='weighted')))\n",
    "print(\"Recall Score: \\t\\t {0:.4f}\".format(recall_score(y_test, clf.predict(X_std_test), average='weighted')))\n",
    "print(\"F1 Score: \\t\\t {0:.4f}\".format(f1_score(y_test, clf.predict(X_std_test), average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      0.82      0.90        11\n",
      "          2       0.82      1.00      0.90         9\n",
      "\n",
      "avg / total       0.95      0.93      0.93        30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_std_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
